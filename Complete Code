#ensemble method python code for object detection using YOLO V4 and faster RCNN with adaboost classifier.

import cv2
import numpy as np
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from keras.models import load_model

# Load the YOLO V4 and Faster R-CNN models
yolo_model = load_model('yolo_v4.h5')
frcnn_model = load_model('faster_rcnn.h5')

# Define the positive and negative image labels
pos_label = 1
neg_label = 0

# Extract features from the positive and negative images using Haar cascades
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

# Define the number of weak classifiers and the maximum depth of the decision trees
n_estimators = 50
max_depth = 1

# Create the decision tree classifier for the AdaBoost algorithm
dt = DecisionTreeClassifier(max_depth=max_depth)

# Create the AdaBoost classifier
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=n_estimators)

# Load the test image for detection
test_img = cv2.imread('test.jpg')

# Detect objects in the test image using YOLO V4
yolo_results = yolo_model.predict(test_img)

# Detect objects in the test image using Faster R-CNN
frcnn_results = frcnn_model.predict(test_img)

# Combine the results from YOLO V4 and Faster R-CNN
results = np.concatenate((yolo_results, frcnn_results), axis=1)

# Create training data and labels for the AdaBoost classifier
data = []
labels = []

for (x, y, w, h) in results:
    roi = test_img[y:y+h, x:x+w]
    roi_features = cv2.resize(roi, (24, 24))
    roi_features = roi_features.reshape(1, -1)
    prediction = ada.predict(roi_features)
    if prediction == pos_label:
        data.append(roi_features)
        labels.append(pos_label)

# Convert the training data and labels to numpy arrays
data = np.array(data)
labels = np.array(labels)

# Train the AdaBoost classifier on the training data and labels
ada.fit(data.reshape(data.shape[0], -1), labels)

# Detect objects in the test image using the ensemble method
test_features = face_cascade.detectMultiScale(test_img)
for (x, y, w, h) in test_features:
    roi = test_img[y:y+h, x:x+w]
    roi_features = cv2.resize(roi, (24, 24))
    roi_features = roi_features.reshape(1, -1)
    prediction = ada.predict(roi_features)
    if prediction == pos_label:
        cv2.rectangle(test_img, (x, y), (x+w, y+h), (0, 255, 0), 2)
    else:
        cv2.rectangle(test_img, (x, y), (x+w, y+h), (0, 0, 255), 2)

# Show the test image with the detected objects
cv2.imshow('Object Detection', test_img)
cv2.waitKey(0)
cv2.destroyAllWindows()


#CNN python code with Adaboost for text extraction  in an image


import cv2
import numpy as np
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Load the image and convert it to grayscale
img = cv2.imread("image.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Extract features using a CNN
model = tf.keras.Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=gray.shape),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(np.array([gray]), np.array([1]), epochs=10)

# Use Adaboost to improve performance
X_train, X_test, y_train, y_test = train_test_split(np.array([gray]), np.array([1]), test_size=0.2, random_state=42)
clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=0.1)
clf.fit(X_train.reshape(X_train.shape[0], -1), y_train)
accuracy = clf.score(X_test.reshape(X_test.shape[0], -1), y_test)
print("Accuracy: ", accuracy)

# Extract text regions from the image
text_regions = []
for x in range(0, gray.shape[1], 50):
    for y in range(0, gray.shape[0], 50):
        region = gray[y:y+50, x:x+50]
        if clf.predict(region.reshape(1, -1)):
            text_regions.append(region)
            
# Display the text regions
for region in text_regions:
    cv2.imshow("Text region", region)
    cv2.waitKey(0)


#STACKING THE ABOVE USED ALGORITHMS TOGETHER

import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from mlxtend.classifier import StackingClassifier

# Load the dataset 
images = load_image()
X, y = image.data, image.target

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define the base classifiers
rf = RandomForestClassifier(n_estimators=100, random_state=42)
gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
lr = LogisticRegression(random_state=42)
knn = KNeighborsClassifier()
mlp = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42)

# Define the meta-classifier
meta_clf = LogisticRegression(random_state=42)

# Define the stacking classifier
stack_clf = StackingClassifier(classifiers=[rf, gb, lr, knn, mlp], 
                               meta_classifier=meta_clf)

# Fit the stacking classifier on the training data
stack_clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = stack_clf.predict(X_test)

# Evaluate the accuracy of the stacking classifier
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy: {:.2f}'.format(accuracy))

